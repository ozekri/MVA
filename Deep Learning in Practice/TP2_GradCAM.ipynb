{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors :\n",
    "\n",
    "**BENYAMINA Elyas**\n",
    "\n",
    "**ZEKRI Oussama**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "v8iqmMIpXP09"
   },
   "source": [
    "## Visualization of CNN: Grad-CAM\n",
    "* **Objective**: Convolutional Neural Networks are widely used on computer vision. It is powerful for processing grid-like data. However we hardly know how and why it works, due to the lack of decomposability into individually intuitive components. In this assignment, we use Grad-CAM, which highlights the regions of the input image that were important for the neural network prediction.\n",
    "\n",
    "\n",
    "* NB: if `PIL` is not installed, try `conda install pillow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "v8iqmMIpXP09"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import urllib.request\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "v8iqmMIpXP09"
   },
   "source": [
    "### Download the Model\n",
    "We provide you a pretrained model `ResNet-34` for `ImageNet` classification dataset.\n",
    "* **ImageNet**: A large dataset of photographs with 1 000 classes.\n",
    "* **ResNet-34**: A deep architecture for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "v8iqmMIpXP09"
   },
   "outputs": [],
   "source": [
    "resnet34 = models.resnet34(weights='ResNet34_Weights.IMAGENET1K_V1')  # New PyTorch interface for loading weights!\n",
    "resnet34.eval() # set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "v8iqmMIpXP09"
   },
   "source": [
    "![ResNet34](https://miro.medium.com/max/1050/1*Y-u7dH4WC-dXyn9jOG4w0w.png)\n",
    "\n",
    "\n",
    "Input image must be of size (3x224x224). \n",
    "\n",
    "First convolution layer with maxpool. \n",
    "Then 4 ResNet blocks. \n",
    "\n",
    "Output of the last ResNet block is of size (512x7x7). \n",
    "\n",
    "Average pooling is applied to this layer to have a 1D array of 512 features fed to a linear layer that outputs 1000 values (one for each class). No softmax is present in this case. We have already the raw class score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "v8iqmMIpXP09"
   },
   "outputs": [],
   "source": [
    "classes = pickle.load(urllib.request.urlopen('https://gist.githubusercontent.com/yrevar/6135f1bd8dcf2e0cc683/raw/d133d61a09d7e5a3b36b8c111a8dd5c4b5d560ee/imagenet1000_clsid_to_human.pkl'))\n",
    "\n",
    "##classes is a dictionary with the name of each class \n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "v8iqmMIpXP09"
   },
   "source": [
    "### Input Images\n",
    "We provide you 20 images from ImageNet (download link on the webpage of the course or download directly using the following command line,).<br>\n",
    "In order to use the pretrained model resnet34, the input image should be normalized using `mean = [0.485, 0.456, 0.406]`, and `std = [0.229, 0.224, 0.225]`, and be resized as `(224, 224)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "v8iqmMIpXP09"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(dir_path):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    # Note: If the inverse normalisation is required, apply 1/x to the above object\n",
    "    \n",
    "    dataset = datasets.ImageFolder(dir_path, transforms.Compose([\n",
    "            transforms.Resize(256), \n",
    "            transforms.CenterCrop(224), # resize the image to 224x224\n",
    "            transforms.ToTensor(), # convert numpy.array to tensor\n",
    "            normalize])) #normalize the tensor\n",
    "\n",
    "    return (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "v8iqmMIpXP09"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "if not os.path.exists(\"data/TP2_images\"):\n",
    "    os.mkdir(\"data/TP2_images\")\n",
    "    !cd data/TP2_images && wget \"https://www.lri.fr/~gcharpia/deeppractice/2023/TP2/TP2_images.zip\" && unzip TP2_images.zip\n",
    "\n",
    "dir_path = \"data/\" \n",
    "dataset = preprocess_image(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "v8iqmMIpXP09"
   },
   "outputs": [],
   "source": [
    "# show the orignal image \n",
    "index = 5\n",
    "input_image = Image.open(dataset.imgs[index][0]).convert('RGB')\n",
    "plt.imshow(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_ktag": "v8iqmMIpXP09"
   },
   "outputs": [],
   "source": [
    "output = resnet34(dataset[index][0].view(1, 3, 224, 224))\n",
    "values, indices = torch.topk(output, 3)\n",
    "print(\"Top 3-classes:\", indices[0].numpy(), [classes[x] for x in indices[0].numpy()])\n",
    "print(\"Raw class scores:\", values[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "v8iqmMIpXP09"
   },
   "source": [
    "### Grad-CAM \n",
    "* **Overview:** Given an image, and a category (‘tiger cat’) as input, we forward-propagate the image through the model to obtain the `raw class scores` before softmax. The gradients are set to zero for all classes except the desired class (tiger cat), which is set to 1. This signal is then backpropagated to the `rectified convolutional feature map` of interest, where we can compute the coarse Grad-CAM localization (blue heatmap).\n",
    "\n",
    "\n",
    "* **To Do**: Define your own function Grad_CAM to achieve the visualization of the given images. For each image, choose the top-3 possible labels as the desired classes. Compare the heatmaps of the three classes, and conclude. \n",
    "\n",
    "\n",
    "* **To be submitted within 2 weeks**: this notebook, **cleaned** (i.e. without results, for file size reasons: `menu > kernel > restart and clean`), in a state ready to be executed (if one just presses 'Enter' till the end, one should obtain all the results for all images) with a few comments at the end. No additional report, just the notebook!\n",
    "\n",
    "\n",
    "* **Hints**: \n",
    " + We need to record the output and grad_output of the feature maps to achieve Grad-CAM. In pytorch, the function `Hook` is defined for this purpose. Read the tutorial of [hook](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks) carefully.\n",
    " + More on [autograd](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html) and [hooks](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks)\n",
    " + The pretrained model resnet34 doesn't have an activation function after its last layer, the output is indeed the `raw class scores`, you can use them directly. \n",
    " + The size of feature maps is 7x7, so your heatmap will have the same size. You need to project the heatmap to the resized image (224x224, not the original one, before the normalization) to have a better observation. The function [`torch.nn.functional.interpolate`](https://pytorch.org/docs/stable/nn.functional.html?highlight=interpolate#torch.nn.functional.interpolate) may help.  \n",
    " + Here is the link of the paper [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/pdf/1610.02391.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "v8iqmMIpXP09"
   },
   "source": [
    "Class: ‘pug, pug-dog’ | Class: ‘tabby, tabby cat’\n",
    "- | - \n",
    "![alt](https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/dog.jpg)| ![alt](https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/cat.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cam_heatmap_input(indice_category, indice_image):\n",
    "    # Lists and functions that will track and store output and gradients.\n",
    "    # We track the values at the output of the conv2 layer and at the input and the output of the bn2 layer\n",
    "    input_list=[]\n",
    "    output_list=[]\n",
    "    grad_input_list=[]\n",
    "    grad_output_list=[]\n",
    "    grad_output_list_bn2=[]\n",
    "    output_list_bn2=[]\n",
    "    \n",
    "    def fw_hook(self, input, output):\n",
    "        output_list.append(output)\n",
    "    def fw_hook2(self, input, output):\n",
    "        input_list.append(input)\n",
    "        output_list_bn2.append(output)\n",
    "    def bw_hook(self, grad_input, grad_output):\n",
    "        grad_output_list.append(grad_output)\n",
    "    def bw_hook2(self, grad_input, grad_output):\n",
    "        grad_input_list.append(grad_input)\n",
    "        grad_output_list_bn2.append(grad_output)\n",
    "\n",
    "    resnet34.layer4[2].bn2.register_forward_hook(fw_hook2)\n",
    "    resnet34.layer4[2].bn2.register_backward_hook(bw_hook2)\n",
    "    resnet34.layer4[2].conv2.register_forward_hook(fw_hook)\n",
    "    resnet34.layer4[2].conv2.register_backward_hook(bw_hook)\n",
    "    \n",
    "    #Generating the output and performing backpropogation only for the class we are working in\n",
    "    resnet34.zero_grad()\n",
    "    input_image=dataset[indice_image][0].view(1, 3, 224, 224)\n",
    "    output = resnet34(input_image)\n",
    "    output_category=output[:,indice_category]\n",
    "    output_category.backward(retain_graph=True) \n",
    "\n",
    "    # We use the gradients at the output of the conv2 layer and the values at the output of the bn2 layer\n",
    "    # One may try to use the values at the input of the bn2 layer (=output of the conv2 layers) to have different but accurate results\n",
    "    #grad_list = grad_input_list[0][0][0]\n",
    "    grad_list = grad_output_list[0][0][0]\n",
    "    #grad_list = grad_output_list_bn2[0][0][0]\n",
    "\n",
    "    #image_list = input_list[0][0][0]\n",
    "    image_list = output_list[0][0]\n",
    "    #image_list = output_list_bn2[0][0]\n",
    "\n",
    "    # For each of the 512 maps, we compute the mean of the gradients related\n",
    "    grad_list_mean = torch.mean(grad_list,axis=(1,2))\n",
    "    \n",
    "    # We multiply each map by its related mean\n",
    "    heatmap = torch.zeros(grad_list[0,:,:].shape)\n",
    "    for i in range(grad_list_mean.shape[0]):\n",
    "        heatmap += grad_list_mean[i] * image_list[i,:,:]\n",
    "    \n",
    "    # We perform relu and resize the image\n",
    "    heatmap_relu = nn.ReLU()(heatmap)\n",
    "    heatmap_resized = F.interpolate(heatmap_relu.view(1, 1, 7, 7), size=(224, 224), mode='bilinear')[0,0]\n",
    "    heatmap_resized = heatmap_resized/torch.max(heatmap_resized)\n",
    "    return heatmap_resized.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_to_image(heatmap, img):\n",
    "    # We create a heatmap with openCV\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET) \n",
    "    # We apply the heatmap to the image\n",
    "    img_heatmap = (np.float32(img)/255 + np.float32(heatmap)/255)\n",
    "    img_heatmap = (img_heatmap-np.min(img_heatmap))/(np.max(img_heatmap)-np.min(img_heatmap))\n",
    "    #img_heatmap = np.uint8(255*img_heatmap)\n",
    "    return img_heatmap[:, :, ::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "v8iqmMIpXP09"
   },
   "source": [
    "### Complementary questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "v8iqmMIpXP09"
   },
   "source": [
    "##### Try GradCAM on others convolutional layers, describe and comment the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cam_heatmap_input_bn2(indice_category, indice_image):\n",
    "    # Lists and functions that will track and store output and gradients.\n",
    "    # We track the values at the output of the conv2 layer and at the input and the output of the bn2 layer\n",
    "    input_list=[]\n",
    "    output_list=[]\n",
    "    grad_input_list=[]\n",
    "    grad_output_list=[]\n",
    "    grad_output_list_bn2=[]\n",
    "    output_list_bn2=[]\n",
    "    def fw_hook(self, input, output):\n",
    "        output_list.append(output)\n",
    "    def fw_hook2(self, input, output):\n",
    "        input_list.append(input)\n",
    "        output_list_bn2.append(output)\n",
    "    def bw_hook(self, grad_input, grad_output):\n",
    "        grad_output_list.append(grad_output)\n",
    "    def bw_hook2(self, grad_input, grad_output):\n",
    "        grad_input_list.append(grad_input)\n",
    "        grad_output_list_bn2.append(grad_output)\n",
    "\n",
    "    resnet34.layer4[2].bn2.register_forward_hook(fw_hook2)\n",
    "    resnet34.layer4[2].bn2.register_backward_hook(bw_hook2)\n",
    "    resnet34.layer4[2].conv2.register_forward_hook(fw_hook)\n",
    "    resnet34.layer4[2].conv2.register_backward_hook(bw_hook)\n",
    "    \n",
    "    #Generating the output and performing backpropogation only for the class we are working in\n",
    "    resnet34.zero_grad()\n",
    "    input_image=dataset[indice_image][0].view(1, 3, 224, 224)\n",
    "    output = resnet34(input_image)\n",
    "    output_category=output[:,indice_category]\n",
    "    output_category.backward(retain_graph=True) \n",
    "\n",
    "    # We use the gradients at the output of the conv2 layer and the values at the output of the bn2 layer\n",
    "    # One may try to use the values at the input of the bn2 layer (=output of the conv2 layers) to have different but accurate results\n",
    "    #grad_list = grad_input_list[0][0][0]\n",
    "    #grad_list = grad_output_list[0][0][0]\n",
    "    grad_list = grad_output_list_bn2[0][0][0]\n",
    "\n",
    "    #image_list = input_list[0][0][0]\n",
    "    #image_list = output_list[0][0]\n",
    "    image_list = output_list_bn2[0][0]\n",
    "\n",
    "    # For each of the 512 maps, we compute the mean of the gradients related\n",
    "    grad_list_mean = torch.mean(grad_list,axis=(1,2))\n",
    "    \n",
    "    # We multiply each map by its related mean\n",
    "    heatmap = torch.zeros(grad_list[0,:,:].shape)\n",
    "    for i in range(grad_list_mean.shape[0]):\n",
    "        heatmap += grad_list_mean[i] * image_list[i,:,:]\n",
    "    \n",
    "    # We perform relu and resize the image\n",
    "    heatmap_relu = nn.ReLU()(heatmap)\n",
    "    heatmap_resized = F.interpolate(heatmap_relu.view(1, 1, 7, 7), size=(224, 224), mode='bilinear')[0,0]\n",
    "    heatmap_resized = heatmap_resized/torch.max(heatmap_resized)\n",
    "    return heatmap_resized.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for indice_image in range(20):\n",
    "    input_image=dataset[indice_image][0].view(1, 3, 224, 224)\n",
    "    output = resnet34(input_image)\n",
    "    _, indices = torch.topk(output,3)\n",
    "    \n",
    "    # We use convert('RGB') to avoid failure because there is one grey\n",
    "    img = np.asarray(Image.open(dataset.imgs[indice_image][0]).convert('RGB'))\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    \n",
    "    f, ax = plt.subplots(1,4,figsize=(20,5))\n",
    "    ax[0].imshow(img)\n",
    "\n",
    "    for i in range(3):\n",
    "        heatmap = grad_cam_heatmap_input(indices[0].numpy()[i], indice_image)\n",
    "        img_with_heatmap = heatmap_to_image(heatmap, img)\n",
    "        ax[i+1].imshow(img_with_heatmap)\n",
    "        ax[i+1].set_title(classes[indices[0].numpy()[i]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "v8iqmMIpXP09"
   },
   "source": [
    "##### What are the principal contributions of GradCAM (the answer is in the paper) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_ktag": "v8iqmMIpXP09"
   },
   "source": [
    "As mentionned in the paper, Grad-CAM is a localization technique that provides visual exaplanation for a CNN-based network. It means that it gave access for each pixel a score of importance in the final output. It can be applied to classification, captioning and VQA models, and without changing it or retraining it.\n",
    "We worked on classification models in this lab. \n",
    "Graphically, we can see which areas were used to provide the prediction of a class.\n",
    "This technque is important as it allow us to track wrong predictions of a model and to understand why it has failed.\n",
    "\n",
    "Let us comment our results.\n",
    "\n",
    "We followed the hints, and we used the function Hook to track outputs and gradients. We noticed that if we use the values (computed during the forward steps) in the output of the bn2 layer (which follows the last convulational layer), we obtained different results that when we use the values in the input of this layer (which are the same as the values of the output of the conv layer).\n",
    "The results seems to be a little bit better in this case. \n",
    "\n",
    "We notice that for most of the cases, the three most probable classes are similar (different type of a same animal) and the ares used for the prediction are very similar.\n",
    "\n",
    "Grad-CAM serves as a valuable tool for gaining insight into the inner workings of the network and its visual focus during prediction. It helps in understanding the variations in predictions and their correlation with specific features observed by the network in the input image. In fact in our examples, Grad_Cam provided clarity on how the network distinguishes between animal species or between species of the same family."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "kfiletag": "v8iqmMIpXP09",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
